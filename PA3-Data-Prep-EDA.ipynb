{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# PA3 Data Prep and Exploratory Data Analysis (75 pts)\n",
    "\n",
    "## Learner Objectives\n",
    "At the conclusion of this programming assignment, participants should be able to:\n",
    "* Write Markdown and code cells in Jupyter Notebook\n",
    "* Type set equations using Latex\n",
    "* Use `matplotlib` to visualize data\n",
    "* Tell a data science story that involves\n",
    "    * Cleaning/preparing data\n",
    "    * Computing summary statistics\n",
    "    * Interpreting visualizations\n",
    "    * Making predictions using simple linear regression\n",
    "\n",
    "## Prerequisites\n",
    "Before starting this programming assignment, participants should be able to:\n",
    "* Use Python for data analysis\n",
    "* Model a 2D table with a custom class\n",
    "\n",
    "## Acknowledgments\n",
    "Content used in this assignment is based upon information in the following sources:\n",
    "* Auto dataset: Dr. Shawn Bowers' Data Mining HW1&2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github Classroom Setup\n",
    "For this assignment, you will use GitHub Classroom to create a private code repository to track code changes and submit your assignment. Open this PA3 link to accept the assignment and create a private repository for your assignment in Github classroom: https://classroom.github.com/a/mcflZkMB\n",
    "\n",
    "Your repo, for example, will be named GonzagaCPSC322/pa3-yourusername (where yourusername is your Github username). I highly recommend committing/pushing regularly so your work is always backed up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Requirements\n",
    "For this programming assignment, we are going to perform data preparation and exploratory data analysis (EDA) of a real-world auto dataset. The analysis of the dataset will be in `AutoData.ipynb` and common, re-usable utility code will go in .py modules (e.g., `mypytable.py`, `utils.py`, `plot_utils.py`, etc.). \n",
    "\n",
    "### Auto Dataset\n",
    "We will use `MyPyTable` objects to load the [auto-mpg.txt and auto-prices.txt files](https://github.com/GonzagaCPSC322/PAs/tree/master/files). These files contain information about cars manufactured and sold in the 1970s.\n",
    "\n",
    "The attributes of `auto-mpg.txt` are: \n",
    "* mpg (miles per gallon)\n",
    "* cylinders\n",
    "* displacement\n",
    "* horsepower\n",
    "* weight\n",
    "* acceleration\n",
    "* model year\n",
    "* origin (1=US, 2=Europe, 3=Japan)\n",
    "* car name\n",
    "\n",
    "The attributes of `auto-prices.txt` are: \n",
    "* car name\n",
    "* model year\n",
    "* msrp (manufacturer's suggested retail price)\n",
    "\n",
    "### General Notes\n",
    "Notes regarding `MyPyTable`:\n",
    "* Copy your `MyPyTable` from PA2\n",
    "* You are encouraged to use `MyPyTable` objects to store the data in tables\n",
    "    * Feel free to add additional methods to the `MyPyTable` class that implement standard table behavior\n",
    "* Do not modify the signatures of existing `MyPyTable` methods from PA2 (future unit tests may call them)\n",
    "\n",
    "Notes regarding your Notebook and utility code:\n",
    "* It is good practice to keep your source code files separated from your data files\n",
    "    * Read in the txt files from the `input_data` folder \n",
    "    * Write txt files to a folder called `output_data`\n",
    "* Your notebook (`AutoData.ipynb`) should be well-organized with interleaved code cells and Markdown cells\n",
    "    * Cells in your notebook should be appropriately labeled using markdown headers for at least the dataset description, dataset loading, preparation/cleaning steps, and EDA/visualization steps\n",
    "    * Your notebook should include an EDA story that includes well-written narrative describing the process you use to prepare/clean the given data, in-line charts (with labels!!), results, and insights you come up with for each step\n",
    "* Strive to write code that is designed/implemented as generic and re-usable functions (that can be used for future programming assignments and data mining tasks)\n",
    "    * Put plotting-related functions in `plot_utils.py`\n",
    "    * Put other functions in `utils.py`\n",
    "* Each formula that you use in your code should be typeset using Latex and described in Markdown\n",
    "    * E.g. formulas used for stats and linear regression/predictions\n",
    "\n",
    "Note: we are learning data science from scratch! The only non-standard Python library you should need to use for this assignment is `tabulate` and `matplotlib`. This means that beyond these libraries, you should not `pip install` any additional libraries beyond what is included in the [continuumio/anaconda3:2024.06-1](https://hub.docker.com/r/continuumio/anaconda3) Docker image and you should not use `pandas/numpy/scipy/`etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš— Auto Dataset Preparation ðŸš—\n",
    "Using the `input_data/auto-mpg.txt` and `input_data/auto-prices.txt` tables, perform the following steps.\n",
    "\n",
    "### Step 1: Duplicates (5 pts)\n",
    "For each dataset, display the number of instances in the dataset and display any duplicates (i.e., instances with the same car name and model year). If duplicates do exist, programmatically resolve the duplicates and write the modified dataset to a file (e.g., `output_data/auto-mpg-no-dups.txt`, `output_data/auto-prices-no-dups.txt`). Write in a Markdown cell the duplicates you found (if any), how you resolved them, and why you resolved them the way you did. The result of this step should display the number of instances in each dataset and the duplicates found in each dataset.\n",
    "\n",
    "### Step 2: Manual Clean (10 pts)\n",
    "Perform a full outer join between your `auto-mpg-no-dups.txt` and `auto-prices-no-dups.txt` tables that have duplicates removed. Use `car name` and `model year` as your key/foreign key columns for the join. Inspect the result of this join to find cases where there are price table rows that do not match mpg table rows. We want to manually (i.e. not with Python) clean up the cases in which these mismatches occur. An example of a mismatch is the audi 100 ls. In the full outer joined table there will be the row: `NA,NA,NA,NA,NA,NA,71,NA,audi 100 ls,3595.0` (note: it is okay if your model year is a float). In the mpg table, there is a row: `24.0,4.0,107.0,90.0,2430.0,14.5,70,2.0,audi 100 ls` and in the prices table there is a row: `audi 100 ls,71,3595.0`. These two rows should match up on car name and model year, but the year is incorrect in the prices table and should be manually corrected so we have a match on full outer join.\n",
    "\n",
    "To make these corrections, manually (e.g. with `cp` at command line or using a graphical user interface such as Windows File Explorer or Mac Finder) make a copy of `auto-mpg-no-dups.txt` called `auto-mpg-clean.txt` and a copy of `auto-prices-no-dups.txt` called `auto-prices-clean.txt`. Put these copies in the `input_data` folder. Open these files in a text editor (e.g. VS Code) and manually edit them as necessary to resolve the issues (e.g., in the case of a mis-matched year like above, or something different like a misspelling). Write in a Markdown cell how and why you resolved these cases the way you did.\n",
    "\n",
    "Finally, for each manually cleaned dataset, count the number of instances (as in Step 1 above).\n",
    "\n",
    "Note: if you did not finish the joins on PA2, I encourage you to finish them now, or skip this step and the next. If you skip, you can use this [`auto-data.txt`](https://github.com/GonzagaCPSC322/PAs/blob/master/files/auto-data.txt) file for subsequent steps that rely on the joined table.\n",
    "\n",
    "### Step 3: Join (fr this time!) (5 pts)\n",
    "Combine the two manually cleaned tables on car name and model year using a full outer join. The resulting joined table should have 10 attributes such that the first 9 attributes are those from `auto-mpg-clean.txt` and the last attribute is the corresponding msrp (price) from `auto-prices-clean.txt`.\n",
    "\n",
    "Write out the result to `output_data/auto-data.txt` and count the number of instances (as in Step 1 above).\n",
    "\n",
    "### Step 4: Summary Stats (5 pts)\n",
    "Compute summary statistics for `auto-data.txt`. For each continuous attribute, compute the minimum, maximum, midpoint (half way between min and max), average, and median values. You can ignore the categorical attributes for now. The result of this step should print the following:\n",
    "\n",
    "```\n",
    "Summary Stats:\n",
    "attribute       min      max       mid        avg    median\n",
    "------------  -----  -------  --------  ---------  --------\n",
    "mpg             ???      ???       ???        ???       ???\n",
    "displacement    ???      ???       ???        ???       ???\n",
    "horsepower      ???      ???       ???        ???       ???\n",
    "weight          ???      ???       ???        ???       ???\n",
    "acceleration    ???      ???       ???        ???       ???\n",
    "model year      ???      ???       ???        ???       ???\n",
    "msrp            ???      ???       ???        ???       ???\n",
    "```\n",
    "\n",
    "Note: the table above is nicely generated using `MyPyTable`'s `pretty_print()` and the [`tabulate`](https://pypi.org/project/tabulate/) library.\n",
    "\n",
    "### Step 5: Missing Values (10 pts)\n",
    "Perform two different techniques to resolve missing values in `auto-data.txt`. Note that there should only be three columns in `auto-data.txt` that contain missing values. \n",
    "1. The first approach should be to remove all instances with missing values.\n",
    "2. The second approach should be to replace missing values with their corresponding attribute's average value.\n",
    "\n",
    "After each approach, count the number of instances and display summary statistics (like Step 4) on the data with missing values resolved. Write the modified datasets to a file (e.g., `output_data/auto-data-removed-NA.txt`, `output_data/auto-data-replaced-NA.txt`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš— Auto Dataset Visualization ðŸš—\n",
    "Using the `output_data/auto-data-replaced-NA.txt` table, perform the following steps.\n",
    "\n",
    "### Step 6 Frequency Diagrams (5 pts)\n",
    "Create a frequency diagram for each of the following attributes (viewed as categorical):\n",
    "1. Cylinders\n",
    "1. Model year\n",
    "1. Origin\n",
    "\n",
    "Each diagram should show the frequency (i.e., total number) of cars per value of the given attribute. Use a basic bar chart to draw your frequency diagrams. Example:\n",
    "\n",
    "<img src=\"https://github.com/GonzagaCPSC322/PAs/raw/master/figures/cylinders_freq_diagram.png\" width=\"400\"/>\n",
    "    \n",
    "### Step 7 Discretization and Frequency Diagrams (10 pts)\n",
    "There is often a need to transform a continuous attribute into a categorical attribute. Use the following two approaches to convert MPG into a categorical attribute.\n",
    "\n",
    "**Approach 1**: The US Department of Energy (DOE) assigns gasoline vehicles a fuel economy rating from 1 (worst) to 10 (best). The ratings are defined in terms of MPG as follows:\n",
    "\n",
    "Rating|MPG\n",
    "-|-\n",
    "10 |â‰¥ 45\n",
    "9 |37â€“44\n",
    "8 |31â€“36\n",
    "7 |27â€“30\n",
    "6 |24â€“26\n",
    "5 |20â€“23\n",
    "4 |17â€“19\n",
    "3 |15â€“16\n",
    "2 |14\n",
    "1 |â‰¤ 13\n",
    "\n",
    "Use these ranges to define category values (denoting rating 1 to 10) for the MPG attribute.\n",
    "\n",
    "**Approach 2**: Create 5 \"equal-width\" bins to generate categories (see the Discretization Lab for details on how to do this). Each bin should divide up the range of MPG values into equal subranges, where value 1 denotes the smallest subrange of values and 5 the largest subrange of values.\n",
    "\n",
    "For each approach, create a corresponding frequency diagram. Each frequency diagram should label bins according to their corresponding ranges (e.g., \"27--30\").\n",
    "\n",
    "### Step 8 Histograms (5 pts)\n",
    "Create a histogram using the `plt.hist()` function showing the values for the following attributes:\n",
    "1. MPG\n",
    "1. Displacement\n",
    "1. Horsepower\n",
    "1. Weight\n",
    "1. Acceleration\n",
    "1. MSRP\n",
    "\n",
    "Use the default of 10 bins. Example:\n",
    "\n",
    "<img src=\"https://github.com/GonzagaCPSC322/PAs/raw/master/figures/mpg_hist.png\" width=\"400\"/>\n",
    "\n",
    "For each attribute, inspect its histogram and comment on its shape as one of the following (or as \"seemingly random\"):\n",
    "\n",
    "![](https://wac-cdn.atlassian.com/dam/jcr:8f510592-413d-49b1-965d-bfa3b1b35a1b/histogram-example-2.png?cdnVersion=2269)\n",
    "\n",
    "(image from https://chartio.com/learn/charts/histogram-complete-guide/)\n",
    "\n",
    "### Step 9 Continuous/Continuous Relationships and Scatter Plots (10 pts)\n",
    "Create scatter plots that compare the following attributes to MPG (i.e. where MPG is the y-axis in each scatter plot):\n",
    "1. Displacement\n",
    "1. Horsepower\n",
    "1. Weight\n",
    "1. Acceleration\n",
    "1. MSRP\n",
    "\n",
    "Then calculate the (least-squares) linear regressions and add the corresponding linear regression lines for comparing each attribute to MPG. Label each plot with the correlation coefficient. Example:\n",
    "\n",
    "<img src=\"https://github.com/GonzagaCPSC322/PAs/raw/master/figures/displacement_scatter.png\" width=\"400\"/>\n",
    "\n",
    "Are there any particularly strong relationships?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (5 pts) Categorical/Continuous Relationships\n",
    "How do the distributions of MPGs look when grouped by model year? Is there a change over time? To answer this, make a chart with model year on the x-axis and MPG on the y-axis. Each model year should have a vertical box and whisker showing the distribution of MPGs for that model year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Assignments\n",
    "1. Turn in your assignment files via a Github Classroom repo. See the \"Github Classroom Setup\" section at the beginning of this document for details on how to do this.\n",
    "    1. Your repo should contain all of the files needed to run and test your solution (e.g. .py file(s), input files, etc.). \n",
    "    1. Double-check that this is the case by \"pretending to be the grader\": clone (or download a zip) your submission repo and run your code in a fresh [continuumio/anaconda3:2024.06-1](https://hub.docker.com/r/continuumio/anaconda3) Docker container like we will when we grade your code.\n",
    "1. Submit this PAâ€™s associated assignment in Canvas to mark your PA as \"done\" and ready for grading. We will then pull your Github repo and grade your PA as soon as possible. The date and time you submit the PA assignment in Canvas will be used for marking your assignment as \"late\" or \"on-time.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Guidelines\n",
    "This assignment is worth 75 points + 5 points bonus. Your assignment will be evaluated based on a successful execution in the [continuumio/anaconda3:2024.06-1](https://hub.docker.com/r/continuumio/anaconda3) Docker container and adherence to the program requirements. We will grade according to the following criteria:\n",
    "* 5 pts for correct step 1 (duplicates)\n",
    "* 10 pts for correct step 2 (manual clean)\n",
    "* 5 pts for correct step 3 (join)\n",
    "* 5 pts for correct step 4 (summary stats)\n",
    "* 10 pts for correct step 5 (missing values)\n",
    "    * 5 pts for removal\n",
    "    * 5 pts for replacement\n",
    "* 5 pts for correct step 6 (freq diagrams)\n",
    "* 10 pts for correct step 7 (discretization + freq diagrams)\n",
    "    * 5 pts for discretization approach 1\n",
    "    * 5 pts for discretization approach 2\n",
    "* 5 pts for correct step 8 (histograms)\n",
    "* 10 pts for correct step 9 (scatter + linear relationships)\n",
    "* 10 pts for adherence to course [coding standard](https://nbviewer.jupyter.org/github/GonzagaCPSC322/PAs/blob/master/Coding%20Standard.ipynb), including data storytelling (narrative is clear and grammatically correct, Notebook is organized with headers, formulas are typeset with Latex, etc.)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
