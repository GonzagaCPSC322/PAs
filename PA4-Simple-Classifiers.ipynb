{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# PA4 Simple Classifiers (75 pts)\n",
    "\n",
    "## Learner Objectives\n",
    "At the conclusion of this programming assignment, participants should be able to:\n",
    "* Implement test-driven development\n",
    "    * Define a test before implementing the unit under test\n",
    "* Implement a linear regression classifier\n",
    "* Implement a kNN classifier\n",
    "* Implement a dummy classifier\n",
    "\n",
    "## Prerequisites\n",
    "Before starting this programming assignment, participants should be able to:\n",
    "* Use Python for data analysis\n",
    "* Calculate a least squares linear regression line\n",
    "* Tell a data science story using Jupyter Notebook\n",
    "* Understand Bramer Sections 3.1 (What is Classification) and 3.3 (Nearest Neighbours Classification)\n",
    "\n",
    "## Acknowledgments\n",
    "Content used in this assignment is based upon information in the following sources:\n",
    "* Part 1 (`mysklearn`): The [Sci-kit Learn](https://scikit-learn.org/stable/) machine learning library for Python\n",
    "* Part 2 (auto dataset classification): Dr. Shawn Bowers' Data Mining HW3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github Classroom Setup\n",
    "For this assignment, you will use GitHub Classroom to create a private code repository to track code changes and submit your assignment. Open this PA4 link to accept the assignment and create a private repository for your assignment in Github classroom: https://classroom.github.com/a/x9ictIhR\n",
    "\n",
    "Your repo, for example, will be named GonzagaCPSC322/pa4-yourusername (where yourusername is your Github username). I highly recommend committing/pushing regularly so your work is always backed up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Requirements\n",
    "This assignment involves implementing simple classification algorithms. It has two main parts:\n",
    "1. `mysklearn`: Test and implement general and re-usable classification algorithms\n",
    "1. Auto Dataset Classification (pa4.ipynb): Write a Jupyter Notebook that uses `mysklearn` to perform data classification tasks on an automobile dataset\n",
    "\n",
    "I highly encourage you to design functions that are generic and re-usable for future programming assignments and data mining tasks.\n",
    "\n",
    "Notes regarding `MyPyTable`:\n",
    "* Copy your `MyPyTable` from PA2/3\n",
    "* You are encouraged (but not required) to use `MyPyTable` to store the data in tables\n",
    "    * Feel free to add additional methods to `MyPyTable` that implement standard table behavior\n",
    "    * Note: `mysklearn` functions and methods do not accept `MyPyTable` objects. Instead, they accept general lists (e.g. `MyPyTable`'s `data` attribute)\n",
    "* Do not modify the signatures of existing `MyPyTable` methods from PA2 (future unit tests may call them)\n",
    "\n",
    "Note: we are learning data science from scratch! The only non-standard Python libraries you should need to use for this assignment are `tabulate`, `numpy` (math functions, random number generation, etc.), and `scipy` (sparingly). This means that beyond these libraries, you should not `pip install` any additional libraries beyond what is included in the [continuumio/anaconda3:2024.06-1](https://hub.docker.com/r/continuumio/anaconda3) Docker image and you should not use `pandas/sklearn/`etc... (exceptions are made for testing purposes only!!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: `mysklearn` (45 pts)\n",
    "Throughout the rest of the semester we are going to be implementing machine learning algorithms and approaches to evaluate these algorithms. The API we are going to (loosely) follow when implementing these algorithms is inspired by the machine learning library, [Sci-kit Learn](https://scikit-learn.org/stable/). As close as reasonably possible, we will follow Sci-kit Learn. I encourage you to read Sci-kit Learn's [An Introduction to Machine Learning with Sci-kit Learn](https://scikit-learn.org/stable/tutorial/basic/tutorial.html) to gain familiarity with how this library is organized. We will organize our algorithm implementations in a package called `mysklearn`. We will test our `mysklearn` functionality using [*test driven development*](https://en.wikipedia.org/wiki/Test-driven_development), meaning we will write a unit test for a function before we write the function under test. \n",
    "\n",
    "Note: for reproducible results with `sklearn`, seed your random number generator appropriately.\n",
    "\n",
    "### Step 1: Implement Unit Tests for `MySimpleLinearRegressionClassifier`\n",
    "We can create a simple classifier using simple (least squares) linear regression! All we have to do is discretize the numeric predicted y values from `MySimpleLinearRegressor` `predict()` and we've got a simple classifier. This classifier only works with a feature matrix with one feature, but that's okay. It's our first classifier :)\n",
    "\n",
    "Finish the unit tests in `test_myclassifiers.py` for the following `MySimpleLinearRegressionClassifier` (Class API design inspiration: Sci-kit Learn's [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)) methods:\n",
    "1. `fit(X_train, y_train)`: Finish the test function `test_simple_linear_regression_classifier_fit()` by implementing the following test case using the y = 2x + some noise data set from class.\n",
    "    * Note: to create a `MySimpleLinearRegressionClassifier` object, you will need to pass a function reference for `discretizer`. Create a function to discretize y values according to the following rule (you will call this function via `self.discretizer` in `predict()` -- see below, you don't need it in `fit()`): \n",
    "        * if y >= 100 then \"high\"\n",
    "        * if y < 100 then \"low\"\n",
    "1. `predict(X_test)`: Finish the test function `test_simple_linear_regression_classifier_predict()` by implementing the following test cases and asserting against desk calculations of the predicted y values\n",
    "    1. Come up with at least two test cases with different values for the slope, the intercept, and a list of test instances\n",
    "    1. Note: See note above about calling `self.discretizer`\n",
    "       \n",
    "### Step 2: Implement Unit Tests for `MyKNeighborsClassifier`\n",
    "Finish the unit tests in `test_myclassifiers.py` for the following `MyKNeighborsClassifier` (Class API design inspiration: Sci-kit Learn's [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)) methods:\n",
    "1. `kneighbors(X_test)`: Finish the test function `test_kneighbors_classifier_kneighbors()` by implementing the following test cases\n",
    "    1. Use the 4 instance training set example traced in class on the iPad, asserting against our desk check\n",
    "    1. Use the 8 instance training set example from ClassificationFun/main.py, asserting against our in-class result\n",
    "    1. Use Bramer 3.6 Self-assessment exercise 2, asserting against exercise solution in Bramer Appendix E\n",
    "1. `predict(X_test)`: Finish the test function `test_kneighbors_classifier_predict()` by implementing the following test cases\n",
    "    1. Use the 4 instance training set example traced in class on the iPad, asserting against our desk check\n",
    "    1. Use the 8 instance training set example from ClassificationFun/main.py, asserting against our in-class result\n",
    "    1. Use Bramer 3.6 Self-assessment exercise 2, asserting against exercise solution in Bramer Appendix E\n",
    "\n",
    "For convenience, I've provided the in-class and Bramer datasets as Python lists below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from in-class #1  (4 instances)\n",
    "X_train_class_example1 = [[1, 1], [1, 0], [0.33, 0], [0, 0]]\n",
    "y_train_class_example1 = [\"bad\", \"bad\", \"good\", \"good\"]\n",
    "\n",
    "# from in-class #2 (8 instances)\n",
    "# assume normalized\n",
    "X_train_class_example2 = [\n",
    "        [3, 2],\n",
    "        [6, 6],\n",
    "        [4, 1],\n",
    "        [4, 4],\n",
    "        [1, 2],\n",
    "        [2, 0],\n",
    "        [0, 3],\n",
    "        [1, 6]]\n",
    "\n",
    "y_train_class_example2 = [\"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"yes\"]\n",
    "\n",
    "# from Bramer\n",
    "header_bramer_example = [\"Attribute 1\", \"Attribute 2\"]\n",
    "X_train_bramer_example = [\n",
    "    [0.8, 6.3],\n",
    "    [1.4, 8.1],\n",
    "    [2.1, 7.4],\n",
    "    [2.6, 14.3],\n",
    "    [6.8, 12.6],\n",
    "    [8.8, 9.8],\n",
    "    [9.2, 11.6],\n",
    "    [10.8, 9.6],\n",
    "    [11.8, 9.9],\n",
    "    [12.4, 6.5],\n",
    "    [12.8, 1.1],\n",
    "    [14.0, 19.9],\n",
    "    [14.2, 18.5],\n",
    "    [15.6, 17.4],\n",
    "    [15.8, 12.2],\n",
    "    [16.6, 6.7],\n",
    "    [17.4, 4.5],\n",
    "    [18.2, 6.9],\n",
    "    [19.0, 3.4],\n",
    "    [19.6, 11.1]]\n",
    "\n",
    "y_train_bramer_example = [\"-\", \"-\", \"-\", \"+\", \"-\", \"+\", \"-\", \"+\", \"+\", \"+\", \"-\", \"-\", \"-\",\\\n",
    "           \"-\", \"-\", \"+\", \"+\", \"+\", \"-\", \"+\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Unit Tests for `MyDummyClassifier`\n",
    "A common approach to evaluate the performance of a classifier is to compare its results to a \"dummy\" (AKA \"baseline\") classifier on the same dataset. While there are several types of dummy classifiers, the most common one is a Zero-R (short for zero rule) classifier which uses a \"most frequent\" strategy: always predict the most common class label in the training set. For example, if 99% of the training set labels are positive instances, it always predicts positive.\n",
    "\n",
    "Finish the unit tests in `test_myclassifiers.py` for the following `MyDummyClassifier` (Class API design inspiration: Sci-kit Learn's [DummyClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)) methods:\n",
    "1. `fit(X_train, y_train)`: Finish the test function `test_dummy_classifier_fit()` by implementing the following test cases\n",
    "    1. `y_train = list(np.random.choice([\"yes\", \"no\"], 100, replace=True, p=[0.7, 0.3]))`\n",
    "        * Note: in this case, \"yes\" should be more frequent than \"no\"\n",
    "    1. `y_train = list(np.random.choice([\"yes\", \"no\", \"maybe\"], 100, replace=True, p=[0.2, 0.6, 0.2]))`\n",
    "        * Note: in this case, \"no\" should be more frequent than \"yes\" and \"maybe\"\n",
    "    1. Come up with at least one more test case, varying the labels and their frequency\n",
    "1. `predict(X_test)`: Finish the test function `test_dummy_classifier_predict()` by implementing the following test cases\n",
    "    1. Use the same test cases as for `fit()`\n",
    "\n",
    "### Step 4: Implement Methods in `myclassifiers.py`\n",
    "Complete the `mysklearn.myclassifiers` methods and test your code for functional correctness against the above unit tests.\n",
    "    \n",
    "Note: you can run all the unit tests in a project with pytest by omitting the name of the test module: `pytest --verbose` Also, the `-s` flag is nice because it allows you to see print statement output from your test execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: ðŸš— Auto Classification ðŸš— (20 pts)\n",
    "Write a Jupyter Notebook (pa4.ipynb) that uses your `mysklearn` package to build simple classifiers for the \"pre-processed\" automobile dataset (auto-data-removed-NA.txt) you created for PA3. In the Notebook, describe the steps, log any assumptions and/or issues you had in doing the steps, and provide insights on the step results. All re-usable utility functions should be separate from your Notebook in an appropriate module.\n",
    "\n",
    "### Step 0 Train/Test Sets: Random Instances\n",
    "First, seed your random number generator. Then select 5 random instances from the dataset to form your test set. The dataset minus these 5 instances form the training set. **Use this same pair of train/test sets for each of the following steps.**\n",
    "\n",
    "### Step 1 Train/Test Sets: Random Instances and Linear Regression\n",
    "Create a **simple linear classifier that predicts Department of Energy (DOE) mpg ratings (AKA rankings) (given in PA3) using vehicle weight**. Your classifier should take one or more instances, compute the predicted MPG values, and then map these to the DOE mpg rating for each corresponding instance. Test your classifier by using the pre-selected 5 random instances from the dataset (see Step 0), predict their corresponding mpg rating, show their actual mpg rating, and report the accuracy of the classifier for the 5 instances. For example:\n",
    "\n",
    "```\n",
    "===========================================\n",
    "STEP 1: Linear Regression MPG Classifier\n",
    "===========================================\n",
    "instance: [4.0, 79.0, 67.0, 1963.0, 15.5, 74.0, 2.0, 'volkswagen dasher', 4100.0]\n",
    "class: 7 actual: 6\n",
    "instance: [8.0, 400.0, 175.0, 4385.0, 12.0, 72.0, 1.0, 'pontiac catalina', 4080.0]\n",
    "class: 1 actual: 2\n",
    "instance: [8.0, 318.0, 210.0, 4382.0, 13.5, 70.0, 1.0, 'dodge d200', 3198.0]\n",
    "class: 1 actual: 1\n",
    "instance: [4.0, 113.0, 95.0, 2228.0, 14.0, 71.0, 3.0, 'toyota corona', 2150.0]\n",
    "class: 6 actual: 6\n",
    "instance: [4.0, 105.0, 75.0, 2230.0, 14.5, 78.0, 1.0, 'dodge omni', 3967.0]\n",
    "class: 6 actual: 7\n",
    "accuracy: 0.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Train/Test Sets: Random Instances and kNN\n",
    "Create a **nearest neighbor classifier that predicts DOE mpg ratings using the number of cylinders, weight, and acceleration\n",
    "attributes for k = 5**. Be sure to normalize the three attribute values and also use the Euclidean\n",
    "distance metric. Similar to Step 1, test your classifier using the pre-selected 5 random instances from the dataset (see Step 0), predict\n",
    "their corresponding mpg rating, show their actual mpg rating, and report the accuracy of the classifier for the 5 instances. For example:\n",
    "\n",
    "```\n",
    "===========================================\n",
    "STEP 2: k=5 Nearest Neighbor MPG Classifier\n",
    "===========================================\n",
    "instance: [4.0, 79.0, 67.0, 1963.0, 15.5, 74.0, 2.0, 'volkswagen dasher', 4100.0]\n",
    "class: 5 actual: 6\n",
    "instance: [8.0, 400.0, 175.0, 4385.0, 12.0, 72.0, 1.0, 'pontiac catalina', 4080.0]\n",
    "class: 1 actual: 2\n",
    "instance: [8.0, 318.0, 210.0, 4382.0, 13.5, 70.0, 1.0, 'dodge d200', 3198.0]\n",
    "class: 1 actual: 1\n",
    "instance: [4.0, 113.0, 95.0, 2228.0, 14.0, 71.0, 3.0, 'toyota corona', 2150.0]\n",
    "class: 6 actual: 6\n",
    "instance: [4.0, 105.0, 75.0, 2230.0, 14.5, 78.0, 1.0, 'dodge omni', 3967.0]\n",
    "class: 6 actual: 7\n",
    "accuracy: 0.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Train/Test Sets: Random Instances and Dummy Classification\n",
    "Create a **dummy classifier that predicts DOE mpg ratings**. Similar to Step 1, test your classifier using the pre-selected 5 random instances from the dataset (see Step 0), predict\n",
    "their corresponding mpg rating, show their actual mpg rating, and report the accuracy of the classifier for the 5 instances. For example:\n",
    "\n",
    "```\n",
    "===========================================\n",
    "STEP 3: (Zero-R) Dummy MPG Classifier\n",
    "===========================================\n",
    "instance: [4.0, 79.0, 67.0, 1963.0, 15.5, 74.0, 2.0, 'volkswagen dasher', 4100.0]\n",
    "class: 4 actual: 6\n",
    "instance: [8.0, 400.0, 175.0, 4385.0, 12.0, 72.0, 1.0, 'pontiac catalina', 4080.0]\n",
    "class: 4 actual: 2\n",
    "instance: [8.0, 318.0, 210.0, 4382.0, 13.5, 70.0, 1.0, 'dodge d200', 3198.0]\n",
    "class: 4 actual: 1\n",
    "instance: [4.0, 113.0, 95.0, 2228.0, 14.0, 71.0, 3.0, 'toyota corona', 2150.0]\n",
    "class: 4 actual: 6\n",
    "instance: [4.0, 105.0, 75.0, 2230.0, 14.5, 78.0, 1.0, 'dodge omni', 3967.0]\n",
    "class: 4 actual: 7\n",
    "accuracy: 0.0\n",
    "```\n",
    "\n",
    "Note: be sure that a test instance can't be its own nearest neighbor!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Classifier Comparison: Linear Regression vs kNN vs Dummy\n",
    "Compare how your three classifiers peformed on the 5 instances from Step 0. Which one performed best, which one performed worst? Try running your Steps 1-3 again with 5 different random instances (i.e. adjust your random seed). Perform this several times. Does different random instances change the results? If so, by how much? How could you improve the reliability of your comparisons? (by the way, classifier evaluation is the topic of our next PA!! ðŸ¤“)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (4 pts)\n",
    "In addition to Zero-R, another common dummy classifier is a Statified Random classifier which uses a \"stratified\" strategy: classify an instance by randomly choosing a class label using the distribution of training set class labels, meaning each class label's probability of being selected is weighted based on its frequency in the training set. For example, if the training `y` labels are: `[\"yes\", \"yes\", \"yes\", \"no\", \"no\"]`, then the probability of the dummy classifer predicting \"yes\" is 60% and the probability of predicting \"no\" is 40%. \n",
    "* (2 pts) Add an attribute to `MyDummyClassifier` that represents the `strategy` the classifier should use (e.g. \"most_frequent\" or \"stratified\"). Update your `__init__()` to add a `strategy` parameter with a default value of \"most_frequent\". Update your `fit()` and `predict()` methods to implement either a Zero-R (\"most_frequent\" strategy) or a Stratified Random (\"stratified\" strategy) classifier based on the value of `strategy`.\n",
    "* (2 pts) Repeat step 3 above using your \"stratified\" strategy. Include these results in your Step 4 write-up.\n",
    "\n",
    "Note: you do not need to add/adjust your `MyDummyClassifier` unit tests for the bonus. Just make sure you set up the `strategy` parameter with a default value of \"most_frequent\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Assignments\n",
    "1. Turn in your assignment files via a Github Classroom repo. See the \"Github Classroom Setup\" section at the beginning of this document for details on how to do this.\n",
    "    1. Your repo should contain all of the files needed to run and test your solution (e.g. .py file(s), input files, etc.). \n",
    "    1. Double-check that this is the case by \"pretending to be the grader\": clone (or download a zip) your submission repo and run your code in a fresh [continuumio/anaconda3:2024.06-1](https://hub.docker.com/r/continuumio/anaconda3) Docker container like we will when we grade your code.\n",
    "1. Submit this PAâ€™s associated assignment in Canvas to mark your PA as \"done\" and ready for grading. We will then pull your Github repo and grade your PA as soon as possible. The date and time you submit the PA assignment in Canvas will be used for marking your assignment as \"late\" or \"on-time.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Guidelines\n",
    "This assignment is worth 75 points + 4 points bonus. Your assignment will be evaluated based on a successful execution in the [continuumio/anaconda3:2024.06-1](https://hub.docker.com/r/continuumio/anaconda3) Docker container and adherence to the program requirements. We will grade according to the following criteria:\n",
    "* 5 pts for correct part 1 step 1 (define `MySimpleLinearRegressionClassifier` unit tests)\n",
    "* 5 pts for correct part 1 step 2 (define `MyKNeighborsClassifier` unit tests)\n",
    "* 5 pts for correct part 1 step 3 (define `MyDummyClassifier` unit tests)\n",
    "* 10 pts for correct part 1 step 4 (finish `MySimpleLinearRegressionClassifier` and pass tests)\n",
    "* 15 pts for correct part 1 step 4 (finish `MyKNeighborsClassifier` and pass tests)\n",
    "* 5 pts for correct part 1 step 4 (finish `MyDummyClassifier` and pass tests)\n",
    "* 5 pts for correct part 2 step 1 (random instances and linear regression classification)\n",
    "* 5 pts for correct part 2 step 2 (random instances and kNN classification)\n",
    "* 5 pts for correct part 2 step 3 (random instances and dummy classification)\n",
    "* 5 pts for a comprehensive and high quality write up for step 4 (compare classifiers)\n",
    "* 10 pts for adherence to course [coding standard](https://nbviewer.jupyter.org/github/GonzagaCPSC322/PAs/blob/master/Coding%20Standard.ipynb), including data storytelling (narrative is clear and grammatically correct, Notebook is organized with headers, formulas are typeset with Latex, code receives a \"good\" `pylint` rating, etc.).\n",
    "    * See [coding standard](https://nbviewer.jupyter.org/github/GonzagaCPSC322/PAs/blob/master/Coding%20Standard.ipynb) for details on how to run `pylint` from command line)\n",
    "    * The `pylint` scoring portion of these 10 points is 5 pts scaled to 1/2 of the `pylint` rating, meaning an 8/10 `pylint` rating would score 4/5 pts (rounded to nearest 1/2 integer)\n",
    "    * Note: there is an import issue caused by various versions of Python packages in the Docker container image we are using. Consequently, I've set `test_myclassifiers.py` to be ignored by pylint (hence the `# pylint: skip-file` at the top of the file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1022/1*AuXDgGrr0wbCoF6KDXXSZQ.jpeg\" width=\"300\"/>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
