{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# PA7 Decision Trees (100 pts)\n",
    "\n",
    "## Learner Objectives\n",
    "At the conclusion of this programming assignment, participants should be able to:\n",
    "* Represent a tree in Python\n",
    "* Implement a decision tree classifier using the TDIDT algorithm\n",
    "* Select an attribute using entropy\n",
    "* Extract rules from a decision tree\n",
    "* Consider the effects of using different feature subsets\n",
    "* (BONUS) Visualize a tree with Graphviz and DOT\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "Before starting this programming assignment, participants should be able to:\n",
    "* Implement test-driven development\n",
    "* Implement a $k$NN and Naive Bayes classifier\n",
    "* Evaluate classifiers using train/test sets\n",
    "* Understand tree representations and common tree traversal algorithms\n",
    "* Understand recursion\n",
    "* Tell a data science story using Jupyter Notebook\n",
    "* Understand Bramer Chapters 4 (Using Decision Trees for Classification), 5 (Decision Tree Induction), and Chapter 9 (Avoiding Overfitting of Decision Trees)\n",
    "\n",
    "## Acknowledgments\n",
    "Content used in this assignment is based upon information in the following sources:\n",
    "* Kaggle's [March Machine Learning Mania 2022](https://www.kaggle.com/c/mens-march-mania-2022/data) competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github Classroom Setup\n",
    "For this assignment, you will use GitHub Classroom to create a private code repository to track code changes and submit your assignment. Open this PA7 link to accept the assignment and create a private repository for your assignment in Github classroom: https://classroom.github.com/a/q1ugjTMx\n",
    "\n",
    "Your repo, for example, will be named GonzagaCPSC322/pa7-yourusername (where yourusername is your Github username). I highly recommend committing/pushing regularly so your work is always backed up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Requirements\n",
    "This assignment involves implementing a decision tree classifier. It has two main parts:\n",
    "1. `mysklearn`: Test and implement a general and re-usable decision tree classifier\n",
    "1. Basketball winner classification (pa7.ipynb): Write a Jupyter Notebook that uses `mysklearn` to perform classification tasks on a NCAA basketball tournament dataset\n",
    "\n",
    "I highly encourage you to design functions that are generic and re-usable for future programming assignments and data mining tasks.\n",
    "\n",
    "Note: we are learning data science from scratch! The only non-standard Python libraries you should need to use for this assignment are `tabulate`, `numpy` (math functions, random number generation, etc.), and `scipy` (sparingly). This means that beyond these libraries, you should not `pip install` any additional libraries beyond what is included in the [continuumio/anaconda3:2024.06-1](https://hub.docker.com/r/continuumio/anaconda3) Docker image and you should not use `pandas/sklearn/`etc... (exceptions are made for testing purposes only!!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: `mysklearn` (65 pts)\n",
    "Our decision tree classifier we are going to implement will:\n",
    "* Be constructed/stored using the nested list representation described in class\n",
    "* Use the entropy-based attribute selection method described in class\n",
    "* Use the majority voting method to deal with clashes described in class\n",
    "* Only work with categorical attributes (you will not need to use any continuous attributes with your decision tree classifier in this assignment)\n",
    "\n",
    "### Step 1: Implement Decision Tree Unit Tests for `myclassifiers.py`\n",
    "Finish the decision tree unit tests in `test_myclassifiers.py` for `MyDecisionTreeClassifier` (Class API design inspiration: Sci-kit Learn's [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html))\n",
    "1. `fit(X_train, y_train)`\n",
    "    1. Finish the test function `test_decision_tree_classifier_fit()` by implementing the following test cases\n",
    "        1. Use the 14 instance \"interview\" training set example traced in class on the iPad, asserting against the tree constructed in [B Attribute Selection (Entropy) Lab Task #1](https://github.com/GonzagaCPSC322/U5-Decision-Trees/blob/master/B%20Attribute%20Selection.ipynb)\n",
    "        1. Use the 15 instance \"iPhone\" training set example from RQ5, asserting against the tree you create with a desk check\n",
    "            * Note: this dataset has clashes in it and when resolving the clashes you will have ties with majority voting... in this case, choose the class label that comes alphabetically first in the attribute domain, that way we all have the same solution tree (as opposed to using a random number generator and flipping a coin, which introduces complexity related to seeding and consistency across solutions)\n",
    "1. `predict(X_test)`\n",
    "    1. Finish the test function `test_decision_tree_classifier_predict()` by implementing the following test cases\n",
    "        1. Use the 14 instance \"interview\" training set example traced in class on the iPad, asserting against the two predictions made in [B Attribute Selection (Entropy) Lab Task #2](https://github.com/GonzagaCPSC322/U5-Decision-Trees/blob/master/B%20Attribute%20Selection.ipynb)\n",
    "        1. For the 15 instance \"iPhone\" training set example from RQ5, use the same two unseen instances from RQ5, asserting against the solution predictions from your own desk check\n",
    "        \n",
    "For convenience, I've provided the \"interview\" dataset as Python lists below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interview dataset\n",
    "header_interview = [\"level\", \"lang\", \"tweets\", \"phd\", \"interviewed_well\"]\n",
    "X_train_interview = [\n",
    "    [\"Senior\", \"Java\", \"no\", \"no\"],\n",
    "    [\"Senior\", \"Java\", \"no\", \"yes\"],\n",
    "    [\"Mid\", \"Python\", \"no\", \"no\"],\n",
    "    [\"Junior\", \"Python\", \"no\", \"no\"],\n",
    "    [\"Junior\", \"R\", \"yes\", \"no\"],\n",
    "    [\"Junior\", \"R\", \"yes\", \"yes\"],\n",
    "    [\"Mid\", \"R\", \"yes\", \"yes\"],\n",
    "    [\"Senior\", \"Python\", \"no\", \"no\"],\n",
    "    [\"Senior\", \"R\", \"yes\", \"no\"],\n",
    "    [\"Junior\", \"Python\", \"yes\", \"no\"],\n",
    "    [\"Senior\", \"Python\", \"yes\", \"yes\"],\n",
    "    [\"Mid\", \"Python\", \"no\", \"yes\"],\n",
    "    [\"Mid\", \"Java\", \"yes\", \"no\"],\n",
    "    [\"Junior\", \"Python\", \"no\", \"yes\"]\n",
    "]\n",
    "y_train_interview = [\"False\", \"False\", \"True\", \"True\", \"True\", \"False\", \"True\", \"False\", \"True\", \"True\", \"True\", \"True\", \"True\", \"False\"]\n",
    "\n",
    "# note: this tree uses the generic \"att#\" attribute labels because fit() does not and should not accept attribute names\n",
    "# note: the attribute values are sorted alphabetically\n",
    "tree_interview = \\\n",
    "        [\"Attribute\", \"att0\",\n",
    "            [\"Value\", \"Junior\", \n",
    "                [\"Attribute\", \"att3\",\n",
    "                    [\"Value\", \"no\", \n",
    "                        [\"Leaf\", \"True\", 3, 5]\n",
    "                    ],\n",
    "                    [\"Value\", \"yes\", \n",
    "                        [\"Leaf\", \"False\", 2, 5]\n",
    "                    ]\n",
    "                ]\n",
    "            ],\n",
    "            [\"Value\", \"Mid\",\n",
    "                [\"Leaf\", \"True\", 4, 14]\n",
    "            ],\n",
    "            [\"Value\", \"Senior\",\n",
    "                [\"Attribute\", \"att2\",\n",
    "                    [\"Value\", \"no\",\n",
    "                        [\"Leaf\", \"False\", 3, 5]\n",
    "                    ],\n",
    "                    [\"Value\", \"yes\",\n",
    "                        [\"Leaf\", \"True\", 2, 5]\n",
    "                    ]\n",
    "                ]\n",
    "            ]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: `fit()` and `predict()`\n",
    "Complete the `mysklearn.myclassifiers.MyDecisionTreeClassifier` methods `fit()` and `predict()` and test your code for functional correctness against the above unit tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: `print_decision_rules()`\n",
    "Finish the `print_decision_rules()` method of `MyDecisionTreeClassifier` that prints out the rules inferred from a decision tree created from a call to `fit()`. Your rules should take the form:\n",
    "\n",
    "```\n",
    "IF att0 == val AND ... THEN class = label\n",
    "IF att1 == val AND ... THEN class = label\n",
    "...\n",
    "```\n",
    "\n",
    "Where \"att0\", \"att1\", ... and \"class\" are replaced with the contextual attribute names and class name if the keyword argument `attribute_names` is not None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: üèÄ Basketball Winner Classification üèÄ (25 pts)\n",
    "Create a decision tree classifier for an NCAA basketball tournament dataset. This is a dataset that I extracted from the [Kaggle March Machine Learning Mania 2022 - Men‚Äôs](https://www.kaggle.com/c/mens-march-mania-2022/data) competition. The original Kaggle dataset has LOTS of data (check it out)!! With some pre-processing of the original dataset, I've created tournament_games2016-2021.csv. This file contains game matchups for 334 tournament games from the last 5 years. Some notes:\n",
    "1. The dataset covers games in the 2016, 2017, 2018, 2019, and 2021 tournaments (there was no tournament in 2020 due to COVID-19)\n",
    "1. There are 67 games in a tournament. 67 games * 5 years = 335 games in total. In 2021, the VCU-Oregon was [ruled a \"no-contest\"](https://www.ncaa.com/news/basketball-men/article/2021-03-20/vcu-oregon-game-ruled-no-contest-due-covid-19-protocols) due to a COVID-19 protocol violation. Therefore, the dataset has 335 - 1 = 334 tournament games\n",
    "1. Each game matchup consists of two teams. Randomly, I assigned one team to be the home (\"H\") team and one team to be the away (\"A\") team to get near equal class distribution.\n",
    "1. Each game matchup includes the game \"Season\", \"HomeTeam\", and \"AwayTeam\" attribute values. I included these to help us humans interpret the game context. These attributes are (likely) too specific to use as features and therefore **you should remove them before classification**.\n",
    "\n",
    "Now for the features... DISCLAIMER: I don't know much about basketball, I don't follow the teams closely, I don't know the important stats, and I have no intuition about this stuff; however, I wanted to make a simple, up-to-date dataset that you could play with (Gonzaga students generally love basketball!!) So, with my limited amount of basketball knowledge (and time), I extracted some VERY simple features from the Kaggle data to describe a game matchup. They are:\n",
    "1. RegularEndingWStreak: which team (\"H\" or \"A\") has the *numerically higher* longest winning streak at the end of this tournament game's corresponding regular season\n",
    "1. RegularSeasonHomePercentageWon: which team (\"H\" or \"A\") has the *numerically higher* home game win percentage during this tournament game's corresponding regular season\n",
    "1. RegularSeasonAwayPercentageWon: which team (\"H\" or \"A\") has the *numerically higher* away game win percentage during this tournament game's corresponding regular season\n",
    "1. RegularSeasonFGPercentMean: which team (\"H\" or \"A\") has the *numerically higher* field goal percentage during this tournament game's corresponding regular season\n",
    "1. RegularSeasonFG3PercentMean: which team (\"H\" or \"A\") has the *numerically higher* 3-pointer percentage during this tournament game's corresponding regular season\n",
    "1. RegularSeasonTOMean: which team (\"H\" or \"A\") has the *numerically higher* turnover percentage during this tournament game's corresponding regular season\n",
    "1. RegularSeasonStlMean: which team (\"H\" or \"A\") has the *numerically higher* accomplished steals percentage during this tournament game's corresponding regular season\n",
    "1. LastOrdinalRank: which team (\"H\" or \"A\") has the *numerically higher* Kenneth Massey ordinal rank at the end of this tournament game's corresponding regular season\n",
    "1. TournamentSeed: which team (\"H\" or \"A\") has the *numerically higher* seed for this tournament\n",
    "1. PlayedBefore: which team (\"H\" or \"A\") won if these two teams played during this tournament game's corresponding regular season (\"N\" if these two teams did not play during this tournament game's corresponding regular season)\n",
    "1. **Winner**: which team (\"H\" or \"A\") won this tournament game\n",
    "    1. This is the **class** we are trying to predict\n",
    "    \n",
    "Since each feature is categorical, you do not need to perform any discretization, etc. The following two steps below have you exploring different subsets. For each step, create a dummy, kNN, Naive Bayes, and decision tree classifier to predict the game winner. Test your classifier using stratified k-fold cross-validation (with k = 10). Format your results as per PA6 and compare your results using:\n",
    "1. Accuracy and error rate\n",
    "1. Precision, recall, and F1 measure\n",
    "1. Confusion matrices\n",
    "\n",
    "### Step 1: Using only the TournamentSeed Feature\n",
    "This provides a baseline set of results. If you always choose the team with the better seed, how often are you right?\n",
    "\n",
    "### Step 2: Using a Feature Subset of your Choosing\n",
    "There are subsets that do *slightly* better than TournamentSeed alone. I challenge you to find them!\n",
    "* Note: because decision trees tend to overfit to training data, I recommend you start with only 2-5 features in your subsets, otherwise your decision rule output will be quite long!\n",
    "* Note: I anticipate the tournament seeds are based on these simple features (plus more, plus expert intuition), so don't expect results much better than using TournamentSeed alone. If you love basketball and data science though, I encourage you to extract additional features you think would be predictive as a fun side project. Perhaps compete in the Kaggle competition next year! ü§ì\n",
    "\n",
    "Lastly, print out the rules inferred from your decision tree classifiers when trained over the entire dataset (as opposed to the cross validation trees) with your \"best\" feature subset. Based on the rules, determine ways your trees can/should be pruned. Note you do not need to write code to perform pruning, just explain how they can be pruned and give the resulting \"pruned\" rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS (8 pts)\n",
    "Finish the `visualize_tree()` method of `MyDecisionTreeClassifier` that generates a Graphviz .dot file and a .pdf file. The .dot file is used to produce a visual PDF representation of the decision tree. The tree should have unique nodes for all attributes and leaves in the tree (this means no node should have more than one incoming edge in the visualization). You can read more about dot and how to do this in the [D Tree Visualization and Pruning](https://github.com/GonzagaCPSC322/U5-Decision-Trees/blob/master/D%20Tree%20Visualization%20and%20Pruning.ipynb) notes on Github. Call this method for the tree fit for part 2 step 2 (e.g. the tree used to print the decision rules). \n",
    "\n",
    "Include the graph generated as a PDF file (produced via dot) in your repo in a directory called `tree_vis`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Assignments\n",
    "1. Turn in your assignment files via a Github Classroom repo. See the \"Github Classroom Setup\" section at the beginning of this document for details on how to do this.\n",
    "    1. Your repo should contain all of the files needed to run and test your solution (e.g. .py file(s), input files, etc.). \n",
    "    1. Double-check that this is the case by \"pretending to be the grader\": clone (or download a zip) your submission repo and run your code in a fresh [continuumio/anaconda3:2024.06-1](https://hub.docker.com/r/continuumio/anaconda3) Docker container like we will when we grade your code.\n",
    "1. Submit this PA‚Äôs associated assignment in Canvas to mark your PA as \"done\" and ready for grading. We will then pull your Github repo and grade your PA as soon as possible. The date and time you submit the PA assignment in Canvas will be used for marking your assignment as \"late\" or \"on-time.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Guidelines\n",
    "This assignment is worth 100 points + 8 points bonus. Your assignment will be evaluated based on a successful execution in the [continuumio/anaconda3:2024.06-1](https://hub.docker.com/r/continuumio/anaconda3) Docker container and adherence to the program requirements. We will grade according to the following criteria:\n",
    "* 20 pts for correct part 1 step 1 (finish iPhone dataset tree and define unit tests)\n",
    "* 25 pts for correct part 1 step 2 (finish `MyDecisionTreeClassifier` and pass `fit()` test)\n",
    "* 10 pts for correct part 1 step 2 (finish `MyDecisionTreeClassifier` and pass `predict()` test)\n",
    "* 10 pts for correct part 1 step 3 (finish `print_decision_rules()`)\n",
    "* 10 pts for correct part 2 (basketball classification algorithm comparison and evaluation)\n",
    "* 10 pts for correct part 2 (basketball classification feature subset comparison and evaluation)\n",
    "* 5 pts for correct part 2 (basketball classification printing of decision rules and pruning commentary)\n",
    "* 10 pts for adherence to course [coding standard](https://nbviewer.jupyter.org/github/GonzagaCPSC322/PAs/blob/master/Coding%20Standard.ipynb), including data storytelling (narrative is clear and grammatically correct, Notebook is organized with headers, formulas are typeset with Latex, code receives a \"good\" `pylint` rating, etc.).\n",
    "    * See [coding standard](https://nbviewer.jupyter.org/github/GonzagaCPSC322/PAs/blob/master/Coding%20Standard.ipynb) for details on how to run `pylint` from command line)\n",
    "    * The `pylint` scoring portion of these 10 points is 5 pts scaled to 1/2 of the `pylint` rating, meaning an 8/10 `pylint` rating would score 4/5 pts (rounded to nearest 1/2 integer)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/512/0*dknuIMqtCrKXBPNN\" width=\"400\"/>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
